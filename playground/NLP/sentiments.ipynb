{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e51f36c-d385-4cb0-b4b8-cf51904da260",
   "metadata": {},
   "source": [
    "# Sentiments Analysis\n",
    "## Based on comments\n",
    "Data Source: https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d9bf0-5925-4906-9db4-6c19ecf0408f",
   "metadata": {},
   "source": [
    "#### 1) Opening the data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54ebcfb-9358-4d57-b19c-3828cf6cd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_folder = './data/'\n",
    "files_paths = [files_folder + i for i in ['amazon_cells_labelled.txt', 'imdb_labelled.txt', 'yelp_labelled.txt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1c960-0d44-476c-9287-0b6216fce790",
   "metadata": {},
   "source": [
    "#### 2) Putting the file data into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44420ae-94e1-484a-b09b-581dc553d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "\n",
    "for file_path in files_paths:\n",
    "    file = open(file_path, 'r')\n",
    "    for line in file:\n",
    "        suffix = -3\n",
    "        if file_path == './data/imdb_labelled.txt':\n",
    "            suffix = -5\n",
    "        \n",
    "        if int(line[-2]):\n",
    "            line = line[:suffix]\n",
    "            positive_reviews.append(line)\n",
    "        else:\n",
    "            line = line[:suffix]\n",
    "            negative_reviews.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc01629-b8b6-4d92-950e-440c74f0f542",
   "metadata": {},
   "source": [
    "#### 3) Doing permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6f83e-d313-4b3a-bc80-c4de0458d2e7",
   "metadata": {},
   "source": [
    "##### Calling library function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ccd5b00-a565-4aa6-9331-ce9b6be0f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea747b8-de15-4303-96e8-cd2aaa65fb9d",
   "metadata": {},
   "source": [
    "##### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d64ef050-63c9-466f-b620-6ac6df5558d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews_permutations = []\n",
    "\n",
    "for i in range(2):\n",
    "    permutations_list = [\" \".join(k) for k in permutations(positive_reviews, i+1)]\n",
    "    positive_reviews_permutations += permutations_list\n",
    "\n",
    "del positive_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c00d3-0d2e-4b60-a50f-34cd9bcf81ed",
   "metadata": {},
   "source": [
    "##### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7d08bb-2d2e-4aa2-93af-c07e80d5273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews_permutations = []\n",
    "\n",
    "for i in range(2):\n",
    "    permutations_list = [\" \".join(k) for k in permutations(negative_reviews, i+1)]\n",
    "    negative_reviews_permutations += permutations_list\n",
    "\n",
    "del negative_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc631007-a2d6-47a5-8339-add27345be5a",
   "metadata": {},
   "source": [
    "#### 4) Building the databases (Train/Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df06d3-1cb2-452f-8c02-cf0eee666392",
   "metadata": {},
   "source": [
    "##### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "406973c8-bcb3-418a-8b6b-5a4fe3f20fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6548a6d-bf3e-40a4-b181-819bf705bfbf",
   "metadata": {},
   "source": [
    "##### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad11a45-bad6-4158-9cc1-0461499517bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_dict = {\n",
    "    'review': positive_reviews_permutations + negative_reviews_permutations,\n",
    "    'sentiment': [1 for i in positive_reviews_permutations] + [0 for i in negative_reviews_permutations]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sentiments_dict)\n",
    "del sentiments_dict, positive_reviews_permutations, negative_reviews_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "221a4775-e712-491f-b177-2042ffc76d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    1687500\n",
       "0    1687500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11cc04-fce2-4f44-a282-8500d9dfbd13",
   "metadata": {},
   "source": [
    "#### 5) Clear texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2744de9-87d7-4c35-bf75-6de639ec95f2",
   "metadata": {},
   "source": [
    "##### Importing library function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9edcf4c6-6c68-4129-8abc-e6c0b6f047bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad823f28-3683-4dda-b1ff-560c5ab7aff8",
   "metadata": {},
   "source": [
    "##### Clear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e45be6eb-b9b5-4576-9972-a9efbe084310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14644ede-bbef-4c77-bdac-934df1c7fa68",
   "metadata": {},
   "source": [
    "##### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39b84eb9-2661-4817-a7bd-986f64009b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>684135</th>\n",
       "      <td>Nice, spicy and tender. The Greek dressing was...</td>\n",
       "      <td>nice, spicy and tender. the greek dressing was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147202</th>\n",
       "      <td>The Ngage is still lacking in earbuds. So medi...</td>\n",
       "      <td>the ngage is still lacking in earbuds. so medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419244</th>\n",
       "      <td>I'm a big fan of this series mostly due to Ann...</td>\n",
       "      <td>i'm a big fan of this series mostly due to ann...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    review  \\\n",
       "684135   Nice, spicy and tender. The Greek dressing was...   \n",
       "2147202  The Ngage is still lacking in earbuds. So medi...   \n",
       "1419244  I'm a big fan of this series mostly due to Ann...   \n",
       "\n",
       "                                              clean_review  \n",
       "684135   nice, spicy and tender. the greek dressing was...  \n",
       "2147202  the ngage is still lacking in earbuds. so medi...  \n",
       "1419244  i'm a big fan of this series mostly due to ann...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_review'] = df['review'].apply(clean_text)\n",
    "df[['review', 'clean_review']].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179724b0-3edb-4096-b49a-ecc369a2afdf",
   "metadata": {},
   "source": [
    "#### 6) Stop Words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e18e67-0a08-4653-8e38-de99565a9227",
   "metadata": {},
   "source": [
    "##### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39b93427-ff44-4f15-b07d-a64874a05440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/stnz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/stnz/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to /home/stnz/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df40633-089d-4c5b-a2a5-371ff49d47bd",
   "metadata": {},
   "source": [
    "##### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03e7ecce-7b27-482e-8dbc-0f8aa0bd49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "usefull_words = {\"no\", \"nor\", \"not\", \"don'\", \"don't\", \"ain\", \"aren\", \"aren't\", \n",
    "                 \"couldn\", \"couldn't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\",\n",
    "                 \"hadn\", \"hadn't\", \"hasn\", \"hasn't\", \"haven't\", \"isn\", \"isn't\",\n",
    "                 \"mightn\", \"mightn't\", \"mustn\", \"mustn't\", \"needn\", \"needn't\",\n",
    "                 \"shan\", \"shan't\", \"shan't\", \"shouldn\", \"shouldn't\", \"wasn\",\n",
    "                 \"wasn't\", \"weren\", \"weren't\", \"won't\", \"wouldn\", \"wouldn't\"}\n",
    "\n",
    "stop_words = set([w for w in stopwords.words('english') if w not in usefull_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89cdb071-d12d-4130-a0cb-8dec6d474bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return \" \".join([word for word in words if word not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c13d2c-40e0-412b-b679-42e27f546eec",
   "metadata": {},
   "source": [
    "##### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0e26a94-e5ac-4bd6-8f3c-02f6c529225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_stopwords'] = df['clean_review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d529d45b-e043-486d-877c-8d7c3952e384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3033409</th>\n",
       "      <td>this is a chilly, unremarkable movie about an ...</td>\n",
       "      <td>chilly , unremarkable movie author living/work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148681</th>\n",
       "      <td>gets a signal when other verizon phones won't....</td>\n",
       "      <td>gets signal verizon phones wo n't . beautiful ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362479</th>\n",
       "      <td>probably not in a hurry to go back. gave up tr...</td>\n",
       "      <td>probably not hurry go back . gave trying eat c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_review  \\\n",
       "3033409  this is a chilly, unremarkable movie about an ...   \n",
       "1148681  gets a signal when other verizon phones won't....   \n",
       "2362479  probably not in a hurry to go back. gave up tr...   \n",
       "\n",
       "                                              no_stopwords  \n",
       "3033409  chilly , unremarkable movie author living/work...  \n",
       "1148681  gets signal verizon phones wo n't . beautiful ...  \n",
       "2362479  probably not hurry go back . gave trying eat c...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['clean_review', 'no_stopwords']].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734d066-ff9d-4fe1-9023-ce9e79f808ad",
   "metadata": {},
   "source": [
    "#### 7) Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cfeb86-0245-4ef2-a8af-3331a7ea6502",
   "metadata": {},
   "source": [
    "I chose stemming beacuse it's computacional complexity is lower than lemming, \n",
    "taking into count that there are many instances of training because of the\n",
    "permutations done to the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57b437-8846-4b6e-9ae4-6da732bc40a6",
   "metadata": {},
   "source": [
    "##### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72228a0f-664d-4463-8282-d51568cc1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a4954-95fb-4092-9770-940de7af39de",
   "metadata": {},
   "source": [
    "##### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dda7b780-747a-4169-a80a-b9dbd15edbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in words])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7318ce8-a9f4-4dda-b860-f876ed5fd7a0",
   "metadata": {},
   "source": [
    "##### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65a7f8e7-a6dd-4f15-9a68-541540c318df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed'] = df['no_stopwords'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02e7b0d8-279a-41a7-964e-88b7ba3fe12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3040742</th>\n",
       "      <td>excellent starter wireless headset . cheap che...</td>\n",
       "      <td>excel starter wireless headset . cheap cheerle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123438</th>\n",
       "      <td>rather enjoyed . 's sad movie , good .</td>\n",
       "      <td>rather enjoy . 's sad movi , good .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283969</th>\n",
       "      <td>acting sucks , music sucks , script sucks , pa...</td>\n",
       "      <td>act suck , music suck , script suck , pace suc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              no_stopwords  \\\n",
       "3040742  excellent starter wireless headset . cheap che...   \n",
       "123438              rather enjoyed . 's sad movie , good .   \n",
       "2283969  acting sucks , music sucks , script sucks , pa...   \n",
       "\n",
       "                                                   stemmed  \n",
       "3040742  excel starter wireless headset . cheap cheerle...  \n",
       "123438                 rather enjoy . 's sad movi , good .  \n",
       "2283969  act suck , music suck , script suck , pace suc...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['no_stopwords', 'stemmed']].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a8774-2c6a-4ad7-b4ee-651069e90b42",
   "metadata": {},
   "source": [
    "#### 8) Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d012a2e-9a1c-4e8c-9b06-4868042cc116",
   "metadata": {},
   "source": [
    "This method is ideal for short text (such as the ones used here).\n",
    "Also it has a lower computing complexity when compared to the TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a86c0f-f823-4feb-8062-da99fe38f18e",
   "metadata": {},
   "source": [
    "##### Importing library function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a200d270-3f38-4c18-96ce-c422af5b18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59be6d2-c29b-4d7b-b1aa-1c04a0379d7a",
   "metadata": {},
   "source": [
    "##### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9bc58-dc01-4717-8108-e1ca32dfa2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(data_treino['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e06647-755a-4e5d-b675-107c3eda596a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
